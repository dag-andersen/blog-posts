# Argo CD and Crossplane together?

Both Argo CD and Crossplane are great tools for managing infrastructure. But what happens when you combine them? Can you use Argo CD to manage the infrastructure managed by Crossplane? The answer is yes, but there are some things to consider.

## Quick recap: What is Crossplane?
Crossplane is a control plane that runs inside Kubernetes, which ensures that the external resources running in the cloud provider are in sync with the state declared in Kubernetes. Crossplane manages the entire lifecycle of the resources declared. All resources managed by Crossplane are declared in manifests stored in Kubernetes.

## Running an Ops Cluster

So in order for Crossplane to manage infrastructure, it needs to be installed on a cluster. This cluster is called the Ops Cluster. The Ops Cluster is where Crossplane is installed and where all the infrastructure is managed.
From the ops cluster, you can manage infrastructure on multiple cloud providers. For clarity lets call the production cluster and the staging cluster the App Clusters. The App Clusters are where the business logic is running. 

< insert image >

The applications that run inside the App Cluster needs to access the infrastructure managed by Crossplane. For example, the application running in the App Cluster needs to access a database. 

When Crossplane creates a resource (e.g., Kubernetes cluster or database) on a cloud provider, it stores the connection details (e.g., access credentials) in the cluster where Crossplane is installed. This is a problem since the connection details are needed in App Clusters, where all the business logic is running. So far, there exists no automated native way of making the Secret available in the App Clusters.

The problem is described on [GitHub](https://github.com/crossplane-contrib/provider-argocd/issues/13).

There are a few ways of overcoming this Secret-distribution challenge. The most naive one would be to create a manual step where the infrastructure team needs to somehow copy the credentials to the production cluster when a new cluster is created. For example, running this line of code for each Secret they want to be copied every time a new cluster is created: `kubectl get secret my-secret-name --context core-cluster --export -o yaml | kubectl apply --context new-app-cluster -f -`

Another way of doing this is using some kind of secret-vault (like HashiCorp Vault50 or GCPâ€™s Secret Manager51), where the credentials are stored when the database is created. Each cloud environment can then read the credentials directly from the vault when needed. This may be considered a better solution and may come with some great benefits (which are beyond the scope of this paper) - but nonetheless, it introduces even more tools/concepts to the infrastructure, which may put even more workload on an infrastructure team.

Therefore, I have created a much simpler automated solution. I have created a fully declarative solution with eventual consistency. I have implemented a service named `manifest-syncer` that runs as a container inside Kubernetes. The purpose of the `manifest-syncer` is to mirror Secrets from its host cluster to target clusters. The `manifest-syncer` is simply deployed to the cluster with its default configuration and is controlled through `CustomResourceDefinitions`. If developers want a Secret to be automatically mirrored/copied from the Core Cluster to, e.g., the production cluster, they just create a manifest describing exactly that (see the following code snippet) and apply it to the Core Cluster.

```yaml
apiVersion: dagandersen.com/v1
kind: Syncer
metadata:
  name: secret-syncer
spec:
  data:
    - sourceName: gcp-database-conn
      sourceNamespace: crossplane-system
      kinds: secret
      targetCluster: gcp-cluster-prod
      targetNamespace: default
```
In this example, it is specified that Secrets named `gcp-database-conn`, in namespace crossplane-system, should be copied to namespace default on the cluster named `gcp-cluster-prod`.


## Getting access to the App Clusters
For the `manifest-syncer` to have access to the App Clusters, it needs a `kubeconfig`. I do not want to provide or generate this `kubeconfig` manually each time I create a new cluster. Instead, I want the `manifest-syncer` to fix this automatically without having to change other services.
The `manifest-syncer` automatically scans its host cluster for Secrets generated by Crossplane containing kubeconfigs. This alone is not enough because it only gives read-access to the clusters. To gain write-access, it scans its host cluster for Secrets generated by ArgoCD with the label: `argocd.argoproj.io/secret-type=cluster`, and from these, it retrieves ArgoCD's access tokens to the App Clusters. The `manifest-syncer` combines the `kubeconfigs` and access tokens and gets write-access to the App Clusters. The `manifest-syncer` repeats this process every 10 seconds to continuously detect when new App Clusters are created.

< insert image>

it is illustrated how the `manifest-syncer` running on the Core Cluster reads ArgoCD's access tokens and the `kubeconfigs` (generated by Crossplane) to copy the database credentials (generated by Crossplane) from the Core Cluster to the App Clusters.

One could argue that it is bad practice for a small infrastructure team to build their own services like manifest- syncer because they need to maintain them themselves - but since this service is self-contained and does not directly interact with other services, it can easily be replaced by a better solution, should a company choose to invest in a more mature solution (like installing a secret-vault).

# Embracing a full declarative approach.

For Argo CD to gain access to an external Cluster (A cluster where Argo CD is not installed on), the ArgoCD CLI is commonly used. 

The Argo CD CLI will use the user's kubeconfig to authenticate with the cluster. 